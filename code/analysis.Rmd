---
title: "Breast Cancer Classification"
author: "Rebecca Barter"
date: "5/7/2020"
output: 
  html_document:
    toc: true
---


The goal of this project is to develop a classifier for whether a diabetic patient will be readmitted within 30 days. The data we will use comes from the [UCI Machine Learning respository](https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008), and consists of patient and hospital information from 130 US hospitals across the years 1999-2008.


# Loading and checking the data


First, load and check the data.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(gt)
library(paletteer)
library(scales)
# load the data
diabetic_orig <- read_csv("../data/raw_data/diabetic_data.csv")
```

Then print the data in the console (or view it using `View()`) to make sure that it looks as you expect.

```{r}
# set width = Inf so the tibble does not suppress columns in the console
print(diabetic_orig, width = Inf)
```

Based on this first view, there are many things that I notice: 

- The variables are mostly categorical and coded using characters such as "Yes", "No", "Steady", "Up", etc

- `age` is coded categorically as intervals. I will convert these to numeric mid-ranges so `[0-10)` becomes `5`, `[10-20)` becomes `15`, etc. This means that whatever model I eventually fit can actually use the fact that `[30-40)` is smaller than `[70-80)` (which it can't when everything is coded categorically).

- Missing values seem to be coded as `?` (at least in the `weight`, `payer_code` and `medical_specialty` variables). There are potentially a lot of missing values in this data. I will remove variables that have more than 30 or 40% of their values missing, depending on ow many variables fall into this category.

- `admission_type_id` is coded numerically, implying an ordering to the options which are actually (1) Emergency, (2) Urgent, (3) Elective, (4) Newborn, (5) Not Available, (6) NULL, (7) Trauma Center, which are not ordered.

- The diagnosis variables `diag_1`, `diag_1`, and `diag_1` are coded using ICD diagnosis codes. No model I fit can make use of these, unless I group them into just a few groups e.g. 20 groups at most. 


The dimension of the data was printed above and appears to be `101,766 x 50`, which means that the data has just over 100,000 encounters and 50 variables.

## Missing values check

One of the tasks I will complete is removing variables that have too many missing values, which are apparently coded as `?`. Below, I use the `map_dbl()` function from the `purrr` R package to calculate the proportion of entries for each column equal to `?`.

```{r}
diabetic_orig %>%
  map_dbl(function(.var) sum(.var == "?") / length(.var)) %>%
  round(2) %>% 
  sort(decreasing = TRUE) %>%
  head(10)
```


If you've never seen map functions before, they come from the `purrr` R package (see my [blog post](http://www.rebeccabarter.com/blog/2019-08-19_purrr/) on purrr). Map functions are methods for applying a function to every column of a data frame (or every entry of a list or vector). The `_dbl` in `map_dbl` means that the output of the function that is applied to each column will be a "double" (a single number).


It seems that there are only three variables (`weight`, `medical_speciality`, and `payer_code`) that have substantial numbers of missing values (40% or more). Since I don't feel comfortable imputing variables that have so few existing values, I will remove these variables.




## Unique values check

Often with these kinds of datasets there are variables that have the same entry in each row. 

```{r}
diabetic_orig %>%
  map_dbl(n_distinct) %>%
  sort()
```

Indeed there are several variables (`examide` and `citoglipton`) that only have one distinct value, so I will remove these too.


## Check ranges

For each of the viables, I also like to check that they all lie within acceptable ranges.

To do this, I will use the `map_df` purrr function, which means the output of the function that I'm applying to each column/feature is a data frame. Specifically, I want the output to be a data frame with two columns (one for the minimum value and one for the maximum value), and I also want a column that contains the name of the feature (which I call "variable" using the `.id = "variable"` argument).

```{r}
diabetic_orig %>% 
  # remove the class and id variables
  select_if(is.numeric) %>%
  select(-encounter_id, -patient_nbr) %>%
  # to each column/variable, create a data frame that contains the max and min 
  # value, and also add a column called "variable" that contains the column 
  # identifier (name)
  map_df(~data.frame(min = min(., na.rm = T), 
                     max = max(., na.rm = T)), .id = "variable")
```

Fortunately, it looks like all of the values are reasonable. If I found that they weren't e.g. there were lots of entries equal to `99`, I might conclude that this was a numeric coding of a missing value, and would convert them to `NA`. Once I had a dataset that had a few of million-year-old people. Rather than choosing to believe that immortals walk among us, I converted their ages to NA.

# Clean data 

I wrote a script `cleaning.R` that cleans the data. I don't include it here because it's quite long, but you can find it on the git repository.



The code below loads the cleaned dataset I created.

```{r}
diabetic_clean <- read_csv("../data/processed_data/diabetic_data_clean.csv")
```


```{r}
# set width = Inf so the tibble does not suppress columns in the console
print(diabetic_clean, width = Inf)
```



# Explore the data

Now that I've loaded in the data, I'm ready to explore it.

First, a few questions arose in my mind, including whether or not there were repeated entries for patients.

The following table counts the number of time each patient ID (`patient_nbr`) appears in the data. There seems to be one patient who has 40 encounters, and a bunch of patients who had over 20 encounters.

```{r}
diabetic_clean %>%
  count(patient_nbr) %>%
  arrange(desc(n))
```

I can also visualize these counts as a histogram, which shows that most patients have only 1 encounter in the data.

```{r fig.align="center", fig.height = 3, fig.width = 6}
diabetic_clean %>%
  count(patient_nbr) %>%
  arrange(desc(n)) %>%
  ggplot() +
  geom_histogram(aes(x = n), binwidth = 1, col = "white") +
  scale_x_continuous("Number of encounters", breaks = 1:40) +
  scale_y_continuous("Number of patients", expand = c(0, 0)) +
  theme_classic() 
```


I'm interested in predicting readmission, so I'm interested to see whether any of the features appear to have any relationship with readmission. I do this by comparing the values of each variable.

Since my outcome is binary (readmitted within 30 days or not), I will use boxplots to examine any difference in distribution of each numeric and continuous feature across. If you were comparing two continuous variables, scatterplots would be more meaningful, but boxplots are great for when one variable is continuous and the other is binary or categorical (with many categories).

The code to make each of these would look as follows:

```{r fig.align = "center", fig.height = 4, fig.width = 5}
diabetic_clean %>%
  ggplot() +
  geom_boxplot(aes(x = as.factor(readmitted), y = time_in_hospital),
               varwidth = TRUE) +
  theme_classic() 
```

I like to vary the width of my boxplots based on how many people/observations are in each group.

There doesn't seem to be a big difference between the amount of time spent in hospital for the readmitted and non-readmitted groups.


Since I don't want to repeat myself by copy and pasting this code, I'm going to write a function that will do it for me. The body of the function `plotBoxplots` below is exactly the same as the code above, except I replaced `y = time_in_hospital` with `y = {{ variable }}`, where `variable` is the argument of the function. These curly braces correspond to "tidy evaluation", that allows for us to use the tidyverse method of referring to columns using their *unquoted* names. 

```{r fig.align = "center", fig.height = 4, fig.width = 5}
plotBoxplots <- function(.var) {
  diabetic_clean %>%
    ggplot() +
    geom_boxplot(aes(x = readmitted, y = .data[[.var]])) +
    theme_classic() +
    ggtitle(.data[[.var]]) +
    scale_x_discrete(NULL, labels = c("Not readmitted", "readmitted"))
}
plotBoxplots("age")
```


So then we can use a map function to generate a list of plots.

```{r}
boxplot_list <- diabetic_clean %>%
  # remove the variables we don't want to plot
  select_if(is.numeric) %>%
  select(-patient_nbr, -encounter_id) %>%
  colnames %>%
  # apply the plotBoxplots function to each column name
  map(plotBoxplots)
```

And we can plot that list of plots using `grid.arrange`.

```{r fig.align="center", fig.height = 6, fig.width=6}
gridExtra::grid.arrange(grobs = boxplot_list)
```


It seems there aren't any stand-out differences between readmitted and non-readmitted patients for any of these variables..


There are also a lot of categorical variables that I'd like to explore. Standard plots like scatterplots and boxplots aren't great at comparing categorical variables with one another, but usually a colored count table will do a pretty good job.

```{r}
printDotPlot <- function(.var) {
  proportions <- diabetic_clean %>%
    count(readmitted, .data[[.var]]) %>%
    drop_na() %>%
    group_by(readmitted) %>%
    mutate(prop = round(n / sum(n), 3)) %>%
    ungroup() 
  proportions %>%
    ggplot() +
    geom_point(aes(x = prop, y = .data[[.var]], col = readmitted),
               size = 3, alpha = 0.8) +
    theme_classic() +
    theme(panel.grid.major.y = element_line(color = "grey50"),
          axis.line = element_blank(),
          legend.position = "top") +
    scale_x_continuous("Proportion of patients", limits = c(0, 1))
}
```


```{r}
dot_plot_list <- diabetic_clean %>% 
  select_if(is.character) %>%
  colnames %>%
  map(printDotPlot)
gridExtra::grid.arrange(grobs = dot_plot_list)
```
  

<!-- # Rough Feature selection -->

<!-- Before I move onto modelling I'm going to do a pretty rough feature selection, where I remove variables that are highly correlated with one another. -->

<!-- To do this, I create a correlation matrix and turn it into a pretty table so I can visualize it. -->

<!-- ```{r} -->
<!-- correlation_matrix <- diabetic_clean %>% -->
<!--   # remove ID variable -->
<!--   select(-id) %>% -->
<!--   # convert class to numeric via factor type -->
<!--   mutate(class = as.numeric(as.factor(class))) %>% -->
<!--   # remove rows with missing values -->
<!--   drop_na() %>% -->
<!--   # create correlation matrix -->
<!--   cor() %>% -->
<!--   # round to 2dp -->
<!--   round(2) -->
<!-- correlation_matrix <- correlation_matrix %>% -->
<!--   as.data.frame() %>% -->
<!--   mutate(variable = colnames(correlation_matrix)) %>% -->
<!--   select(variable, everything()) -->
<!--   # convert to gt table -->
<!-- correlation_matrix %>%   -->
<!--   gt() %>% -->
<!--   # add colors to the table - copy-pasted from the gt documentation ;) -->
<!--   data_color( -->
<!--     columns = 2:11, -->
<!--     colors = col_numeric( -->
<!--       palette = paletteer_d( -->
<!--         palette = "ggsci::red_material" -->
<!--         ) %>% as.character(), -->
<!--       domain = NULL -->
<!--       ) -->
<!--   ) -->
<!-- ``` -->


<!-- Unsurprisingly (based on the boxplots), it seems like `class` is fairly highly correlated with everything. Admittedly this is a fairly rough calculation since `class` is binary, and correlations aren't well-suited to binary variables, but whatever, who cares. -->

<!-- In the interests of not using super highly correlated variables, I'm going to remove one of `cell_size_uniformity` or `cell_shape_uniformity`, since these have a correlation with one another of over 0.9.  -->

<!-- To check these variables, I create a jittered scatterplot -->

<!-- ```{r} -->
<!-- diabetic_clean %>% -->
<!--   ggplot() + -->
<!--   geom_jitter(aes(x = cell_size_uniformity, y = cell_shape_uniformity)) + -->
<!--   theme_classic() + -->
<!--   scale_x_continuous(breaks = 0:10) + -->
<!--   scale_y_continuous(breaks = 0:10) -->
<!-- ``` -->

<!-- The reason that they are so similar to one another appears to be that a lot of the values of both variables are equal to 1.  -->


<!-- They both have a correlation of 0.82 with our responce, `class`, but `cell_shape_uniformity` is slightly less correlated with the other variables (implying it's capturing slightly different information than the other variables), so I'll keep `cell_shape_uniformity` and remove `cell_size_uniformity`. In general, I'd probably remove variables that have a correlation with one another of over 0.8, but that's just an arbitrary cutoff that I could probably lower to 0.7 if I felt like it. -->

<!-- ```{r} -->
<!-- diabetic_clean <- diabetic_clean %>% -->
<!--   select(-cell_size_uniformity) -->
<!-- ``` -->


<!-- That was quick and dirty. I could do fancier things, but I don't have that many features so it doesn't seem necessary. For what it's worth, the fancier things I might do are as follows: -->

<!-- 1. Fit a Random Forest model and grab the variable importance and filter to the top most important features -->

<!-- 1. Fit a Lasso model (which will require parameter tuning), and only keep the features whose coefficients were not shrunk to zero. -->

# Split into training and test sets


```{r}
diabetic_clean <- diabetic_clean %>%
  select(-encounter_id, -patient_nbr)
diabetic_split <- initial_split(diabetic_clean)
```

# Pre-processing

I want to start fitting some models, so I'd like to impute the missing values. Since tidymodels is a thing, we will write a recipe for doing that.

Since the features are all on the same scale, I won't do any normalization or standardization.

```{r}
# define the recipe
diabetic_recipe <-
  # which consists of the formula (outcome ~ predictors)
  recipe(readmitted ~ .,
         data = diabetic_clean) %>%
  # impute the missing values
  step_meanimpute(all_numeric()) %>%
  step_modeimpute(all_nominal()) %>%
   step_log(number_diagnoses, number_emergency, 
                num_procedures, num_medications, 
                num_lab_procedures, number_outpatient) %>%
  step_downsample(readmitted) %>%
  step_nzv(all_predictors()) %>%
  step_dummy(all_nominal(), -readmitted) 
prep(diabetic_recipe, diabetic_clean)
```

## Modeling

I'm going to fit two models: a logistic regression model and a random forest model. I'm not going to tune any parameters for the random forest model, but if you want to see how to do this, check out my [tidymodels blog post](http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/).


```{r}
random_forest_model <-
  # specify that the model is a random forest
  rand_forest() %>%
  # select the engine/package that underlies the model
  set_engine("ranger", importance = "impurity") %>%
  # choose either the continuous regression or binary classification mode
  set_mode("classification") %>%
  metric_set(accuracy)
```


```{r}
logistic_regression_model <-
  logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")
```




Now we can put the recipe and models together into a workflow

```{r}
# RF workflow
random_forest_workflow <- workflow() %>%
  # add the recipe
  add_recipe(diabetic_recipe) %>%
  # add the model
  add_model(random_forest_model)

# LR workflow
logistic_regression_workflow <- workflow() %>%
  # add the recipe
  add_recipe(diabetic_recipe) %>%
  # add the model
  add_model(logistic_regression_model)
```

# Evaluating the models

We can evaluate our models on the test set, first for the random forest fit:

```{r}
random_forest_fit <- random_forest_workflow %>%
  # fit on the training set and evaluate on test set
  last_fit(diabetic_split)

random_forest_fit %>% collect_metrics()

```

and then for the logistic regression fit:

```{r}
logistic_regression_fit <- logistic_regression_workflow %>%
  # fit on the training set and evaluate on test set
  last_fit(diabetic_split)
lr_predictions <- collect_predictions(logistic_regression_fit)

metrics <- metric_set(roc_auc, accuracy, sens, spec)

metrics(lr_predictions, truth = readmitted, estimate = .pred_class, .pred_readmitted)
```

## Plots


```{r}
rf_roc <- random_forest_fit$.predictions[[1]] %>%
  roc_curve(readmitted, .pred_readmitted) %>%
  mutate(model = "RF")
lr_roc <- logistic_regression_fit$.predictions[[1]] %>%
  roc_curve(readmitted, .pred_readmitted) %>%
  mutate(model = "logistic")
rbind(rf_roc, lr_roc) %>%
  ggplot() +
  geom_line(aes(x = 1 - specificity, y = sensitivity, col = model)) +
  geom_abline(intercept = 0, slope = 1, 
              col = "grey40", linetype = "dashed") +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic() +
  coord_fixed()
```



## Interpreting coefficients


```{r}
# fit model to full data
rf_final <- fit(random_forest_workflow, diabetic_clean)
ranger_obj <- pull_workflow_fit(rf_final)$fit
ranger_obj

lr_final <- fit(logistic_regression_workflow, diabetic_clean)
lr_obj <- pull_workflow_fit(lr_final)$fit
summary(lr_obj)$coefficients 
```


```{r}
rf_final <- fit(random_forest_workflow, diabetic_clean)
# extract the logistic regression model object
rf_obj <- pull_workflow_fit(rf_final)$fit
```



```{r}
rf_obj$variable.importance %>% 
  enframe() %>%
  arrange(desc(value)) %>%
  mutate(name = fct_inorder(name)) %>%
  ggplot() +
  geom_col(aes(x = name, y = value),
           color = "white") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  labs(x = NULL, y = "importance")
```


## Conclusion

There a few things I could try to improve the performance of these models, but the fact is most of the time, you have what you have. Don't try to squeeze juice out of a dried up lemon. 


